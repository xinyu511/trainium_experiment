defaults:
  - dataset: cifar10

dataloader:  
  train_batch_size: 128
  val_batch_size: 128
  test_batch_size: 1

classifier: resnet20
pl_model:
  _target_: utils.pl_module.ImageCertifier
  # cert:
  #   _target_: utils.cert_model.CertNet
  #   num_mlps: 128
  #   backbone: 
  #     _target_: torch.hub.load
  #     repo_or_dir: chenyaofo/pytorch-cifar-models
  #     model: cifar10_${classifier}
  #     pretrained: false
  #   clf_path: ./logs/clf/resnet20/cifar10/lightning_logs/version_0/checkpoints/saved_epoch=17.ckpt
  cert:
    _target_: utils.cert_model.FuseNet
    backbone: 
      _target_: torch.hub.load
      repo_or_dir: chenyaofo/pytorch-cifar-models
      model: cifar10_${classifier}
      pretrained: false
    clf_path: ./logs/clf/resnet20/cifar10/lightning_logs/version_0/checkpoints/saved_epoch=18.ckpt

  clf_path: ./logs/clf/resnet20/cifar10/lightning_logs/version_0/checkpoints/saved_epoch=18.ckpt
  snr_range: [0, 40]
  mc_samples: 100
  optimizer:
    _partial_: true
    _target_: torch.optim.Adam
    lr: 1e-3
  scheduler:
    _partial_: true
    _target_: torch.optim.lr_scheduler.MultiStepLR
    gamma: 0.1
    milestones: [75, 150]

trainer:
  log_dir: ./logs/cert/${classifier}/${dataset.name}/
  log_every_n_steps: 10
  accumulate_grad_batches: 1
  gradient_clip_val: 5
  max_epochs: 200
  gpu: 5
  val_monitor: val/loss
  val_check_interval: 1.0
  mode: min
  deterministic: true

hydra:
  run:
    dir: .
  job:
    chdir: false
  output_subdir: null  # prevent auto-creating .hydra in a run dir
